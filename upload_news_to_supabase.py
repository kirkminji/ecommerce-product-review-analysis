
import os
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from supabase import create_client, Client
from tqdm import tqdm
import math
import sys

# .env 파일 로드
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("Error: SUPABASE_URL 또는 SUPABASE_KEY가 .env 파일에 설정되어 있지 않습니다.")
    sys.exit(1)

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

CSV_PATH = "/Users/minzzy/Desktop/statrack/book-review-analysis/analysis/correlation/news_2025_categorized.csv"
TABLE_NAME = "news_2025_categorized"

# 컬럼 매핑 (한글 -> 영문)
COLUMN_MAPPING = {
    '뉴스 식별자': 'news_id',
    '일자': 'news_date',
    '언론사': 'publisher',
    '기고자': 'author',
    '제목': 'title',
    '통합 분류1': 'category_1',
    '통합 분류2': 'category_2',
    '통합 분류3': 'category_3',
    '사건/사고 분류1': 'accident_1',
    '사건/사고 분류2': 'accident_2',
    '사건/사고 분류3': 'accident_3',
    '인물': 'persons',
    '위치': 'locations',
    '기관': 'organizations',
    '키워드': 'keywords',
    '특성추출(가중치순 상위 50개)': 'features',
    '본문': 'content',
    'URL': 'url',
    '분석제외 여부': 'is_excluded',
    '년월': 'year_month',
    '분류텍스트': 'category_text',
    '카테고리': 'category'
}

def upload_news():
    if not os.path.exists(CSV_PATH):
        print(f"Error: 파일을 찾을 수 없습니다: {CSV_PATH}")
        return

    # 청크 사이즈 설정 (본문 내용이 길 수 있으므로 500개씩 배치 처리)
    chunk_size = 500
    
    # 전체 라인 수 확인 (tqdm progress bar를 위함)
    print("파일 읽는 중...")
    try:
        # csv 파일의 대략적인 줄 수를 알고 있으므로 직접 설정 (약 22만 줄)
        total_lines = 220847 
    except:
        total_lines = None
    
    print(f"데이터 업로드 시작: {TABLE_NAME} 테이블로...")
    
    # CSV 파일을 청크 단위로 읽기
    chunks = pd.read_csv(CSV_PATH, chunksize=chunk_size, low_memory=False)
    
    total_uploaded = 0
    
    # tqdm으로 진행 상황 표시
    pbar = tqdm(total=total_lines) if total_lines else tqdm()
    
    for chunk in chunks:
        # 컬럼명 변경
        chunk = chunk.rename(columns=COLUMN_MAPPING)
        
        # 매핑된 컬럼만 선택
        available_cols = [col for col in COLUMN_MAPPING.values() if col in chunk.columns]
        chunk = chunk[available_cols]
        
        # 결측치 처리 (NaN -> None)
        chunk = chunk.replace({np.nan: None})
        
        # numeric 타입 데이터 형변환 (필요시)
        # news_id가 2100601.20250331 형태이므로 문자열로 유지하는 것이 좋음
        if 'news_id' in chunk.columns:
            chunk['news_id'] = chunk['news_id'].astype(str)
            
        # 데이터를 리스트 형태로 변환
        data = chunk.to_dict(orient='records')
        
        # Supabase에 데이터 삽입
        try:
            response = supabase.table(TABLE_NAME).insert(data).execute()
            total_uploaded += len(data)
            pbar.update(len(data))
        except Exception as e:
            pbar.close()
            error_msg = str(e)
            if "relation" in error_msg and "does not exist" in error_msg:
                print(f"\n[오류] '{TABLE_NAME}' 테이블이 존재하지 않습니다.")
                print("Supabase SQL Editor에서 아래 SQL을 실행하여 테이블을 먼저 생성해주세요:")
                print(generate_sql())
            else:
                print(f"\n[오류] 데이터 삽입 중 오류 발생: {e}")
            break
            
    pbar.close()
    print(f"\n완료! 총 {total_uploaded}개의 뉴스를 업로드했습니다.")

def generate_sql():
    """테이블 생성을 위한 SQL 쿼리 생성"""
    sql = f"""
CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
    id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    news_id TEXT,
    news_date DATE,
    publisher TEXT,
    author TEXT,
    title TEXT,
    category_1 TEXT,
    category_2 TEXT,
    category_3 TEXT,
    accident_1 TEXT,
    accident_2 TEXT,
    accident_3 TEXT,
    persons TEXT,
    locations TEXT,
    organizations TEXT,
    keywords TEXT,
    features TEXT,
    content TEXT,
    url TEXT,
    is_excluded TEXT,
    year_month TEXT,
    category_text TEXT,
    category TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- 인덱스 추가 (검색 속도 향상을 위함)
CREATE INDEX IF NOT EXISTS idx_news_date ON {TABLE_NAME} (news_date);
CREATE INDEX IF NOT EXISTS idx_category ON {TABLE_NAME} (category);
"""
    return sql

if __name__ == "__main__":
    upload_news()
