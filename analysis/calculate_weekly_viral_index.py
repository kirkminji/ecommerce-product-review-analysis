#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° ìŠ¤í¬ë¦½íŠ¸
1. DB(Supabase)ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
2. ì£¼ê°„(Weekly) ë‹¨ìœ„ë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ë¥¼ ì§‘ê³„
3. WoW, MA4 í¸ì°¨, Z-Scoreë¥¼ ê²°í•©í•˜ì—¬ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ
4. ë¶„ì„ìš© CSV íŒŒì¼ë¡œ ì €ì¥
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# Supabase ì„¤ì •
try:
    from supabase import create_client, Client
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        SUPABASE_ENABLED = True
    else:
        SUPABASE_ENABLED = False
        print("[ì˜¤ë¥˜] Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
except ImportError:
    SUPABASE_ENABLED = False
    print("[ì˜¤ë¥˜] supabase ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

def fetch_news_data_from_db():
    """Supabase news_2025_categorized í…Œì´ë¸”ì—ì„œ ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©)"""
    if not SUPABASE_ENABLED:
        return None
    
    print("ğŸ“‚ DBì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    table_name = "news_2025_categorized"
    all_data = []
    page_size = 1000
    offset = 0
    
    try:
        while True:
            res = supabase.table(table_name).select('news_date, category').range(offset, offset + page_size - 1).execute()
            data = res.data
            if not data:
                break
            all_data.extend(data)
            offset += page_size
            if len(data) < page_size:
                break
            print(f"  >> ë¡œë”© ì¤‘... ({len(all_data):,}ê°œ)", end="\r")
        
        df = pd.DataFrame(all_data)
        if df.empty:
            print("\n  >> ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        df = df.rename(columns={'news_date': 'date'})
        df['date'] = pd.to_datetime(df['date'])
        print(f"\n  >> ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ê¸°ì‚¬")
        return df
    except Exception as e:
        print(f"\n  >> DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def calculate_weekly_viral_index(df):
    """ì£¼ê°„ ë°”ì´ëŸ´ ì§€ìˆ˜ ê³„ì‚° (ìˆ˜~í™” ê¸°ì¤€, ì¸í”¼ë‹ˆí‹° ë³´ì • ì ìš©)"""
    print("\nğŸ“Š ì£¼ê°„ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ ì¤‘ (ë¬´í•œëŒ€ ê°’ ë³´ì • í¬í•¨)...")
    
    # 1. ì£¼ê°„/ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ ì§‘ê³„
    weekly_counts = df.groupby([pd.Grouper(key='date', freq='W-TUE'), 'category']).size().unstack(level=1).fillna(0)
    
    # 2. ì§€ìˆ˜ êµ¬ì„± ìš”ì†Œ ê³„ì‚° (ì•ˆì •ì„± ê°•í™”)
    # 2-1. WoW (Week-over-Week) ì¦ê°€ìœ¨ - ë¶„ëª¨ì— 1ì„ ë”í•´ 0->N ê¸‰ì¦ ì‹œ inf ë°©ì§€
    prev_counts = weekly_counts.shift(1)
    wow_growth = ((weekly_counts - prev_counts) / (prev_counts + 1)) * 100
    
    # 2-2. 4ì£¼ ì´ë™í‰ê·  ëŒ€ë¹„ í¸ì°¨ - ë¶„ëª¨ì— 1ì„ ë”í•´ inf ë°©ì§€
    ma4 = weekly_counts.rolling(window=4, min_periods=1).mean()
    ma_deviation = ((weekly_counts - ma4) / (ma4 + 1)) * 100
    
    # 2-3. Z-Score
    z_scores = (weekly_counts - weekly_counts.mean()) / (weekly_counts.std() + 1e-9)
    
    # 3. ì¢…í•© ë°”ì´ëŸ´ ì§€ìˆ˜ (ê°€ì¤‘í•©)
    # ê·¹ë‹¨ì ì¸ ì´ìƒì¹˜(Outlier)ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ê° ì§€ìˆ˜ë¥¼ ì ì ˆí•œ ë²”ìœ„ë¡œ í´ë¦¬í•‘(Clipping)
    viral_index = (
        wow_growth.clip(upper=300).fillna(0) * 0.4 +
        ma_deviation.clip(upper=300).fillna(0) * 0.4 +
        z_scores.clip(lower=-3, upper=3).fillna(0) * 10 * 0.2
    )
    
    print(f"  >> ê³„ì‚° ì™„ë£Œ: {len(weekly_counts)}ê°œ ì£¼ì°¨ x {len(weekly_counts.columns)}ê°œ ì¹´í…Œê³ ë¦¬")
    return weekly_counts, viral_index

def main():
    print("=" * 60)
    print("[ì£¼ê°„ ë‰´ìŠ¤ ë°”ì´ëŸ´ ì§€ìˆ˜ ì‚°ì¶œ - ìˆ˜~í™” ê¸°ì¤€]")
    print("=" * 60)
    
    news_df = fetch_news_data_from_db()
    if news_df is None:
        return
    
    counts_df, viral_df = calculate_weekly_viral_index(news_df)
    
    # 3. ë°ì´í„° êµ¬ì¡° ì •ë¦¬
    viral_long = viral_df.stack().reset_index()
    viral_long.columns = ['end_date', 'category', 'viral_index']
    
    counts_long = counts_df.stack().reset_index()
    counts_long.columns = ['end_date', 'category', 'article_count']
    
    result_df = pd.merge(viral_long, counts_long, on=['end_date', 'category'])
    
    # 4. ë² ìŠ¤íŠ¸ì…€ëŸ¬ í…Œì´ë¸”ê³¼ ë§¤ì¹­ì„ ìœ„í•œ ymw ë° bestseller_week í¬ë§· ìƒì„±
    # end_dateê°€ í™”ìš”ì¼ì´ë¯€ë¡œ, ì‹œì‘ì¼ì€ 6ì¼ ì „ì¸ ìˆ˜ìš”ì¼
    result_df['start_date'] = result_df['end_date'] - pd.Timedelta(days=6)
    
    # ymw ìƒì„± ë¡œì§: YYYYMM + (í•´ë‹¹ ì›”ì˜ në²ˆì§¸ ì£¼)
    # ë² ìŠ¤íŠ¸ì…€ëŸ¬ DBì˜ ymw ê·œì¹™ì„ ë”°ë¦„ (ì¢…ë£Œì¼ ê¸°ì¤€)
    def generate_ymw(dt):
        year = dt.year
        month = dt.month
        week = (dt.day - 1) // 7 + 1
        return f"{year}{month:02d}{week}"
    
    result_df['ymw'] = result_df['end_date'].apply(generate_ymw)
    
    result_df['bestseller_week'] = (
        result_df['start_date'].dt.strftime('%Y.%m.%d') + 
        " ~ " + 
        result_df['end_date'].dt.strftime('%Y.%m.%d')
    )
    
    # ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •
    result_df = result_df[['ymw', 'bestseller_week', 'category', 'viral_index', 'article_count', 'start_date', 'end_date']]
    
    # 5. CSV ì €ì¥
    output_dir = "/Users/minzzy/Desktop/statrack/book-review-analysis/analysis"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    output_path = os.path.join(output_dir, "weekly_news_viral_index.csv")
    result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    print("\n" + "=" * 60)
    print(f"âœ… ì‘ì—… ì™„ë£Œ: {output_path}")
    print(f"   - ë°ì´í„° ê¸°ê°„: {result_df['start_date'].min().date()} ~ {result_df['end_date'].max().date()}")
    print(f"   - ì´ ë°ì´í„° í–‰: {len(result_df)}ê°œ")
    print("=" * 60)

if __name__ == "__main__":
    main()
